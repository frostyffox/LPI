# -*- coding: utf-8 -*-
"""practical3_giulia_dirocco_ok.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dFE4PcRTX1q8qEVpMWRONT2wrCYKvSt7

# Machine Learning in Practice

In this session, we will illustrate several useful Machine Learning concepts.

The first thing we will do is to generate a toy classification problem (we already did a regression).

Then we will analyse a real world dataset with machine learning.

We start as usual by doing the imports
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
import matplotlib.pyplot as plt

"""## A dummy classification task

Let us redo most steps of the first practical but in a classification setting!

The new function we want to learn is the arbitrary following one :
$y(x) = round(\frac{1}{1 + e^{(1.3 x^3 - 5 x^2 + 3.6^x + 1.6)}})$

It is already implemented and returns either zero or one.

**Assignment**: Redo the previous steps of generating and plotting the data as well as the underlying relationship.

- Start by ploting the underlying relationship
- Generate toy samples and add noise (use gaussian noise). What do you see when noise level increases?

ðŸ¤­*This is a function that actually outputs either 0 or 1 so it's good for classification.*

I think this is a binary classification problem and i will have to observe how different levels of noise affect the network learning.
"""

# GROUND TRUTH

def base_function(x):
    return np.round(1 / (1 + np.exp(1.3 * x ** 3 - 5 * x ** 2 + 3.6 * x + 1.6)))


low, high = -1, 3
n_points = 200

#define input space: generate inputs and true labels
x_space = np.linspace(low, high, n_points)
y_true = base_function(x_space)

#plotting
plt.plot(x_space, y_true, color='red')
plt.title("The ground truth")
plt.xlabel('x')
plt.ylabel('y')
plt.show()
#add noise
#plotting

#Adding noise

#making observations
n_obs = 200
rng = np.random.default_rng(42)
x_obs = rng.uniform(-1,3,n_obs) #randomly sampled data points
y_clean = base_function(x_obs)

#creating noise
noise_levels = [0.05, 0.3, 0.8]

y_n_1 = y_clean + rng.normal(0, noise_levels[0], size=x_obs.shape)
y_n_2 = y_clean + rng.normal(0, noise_levels[1], size=x_obs.shape)
y_n_3 = y_clean + rng.normal(0, noise_levels[2], size=x_obs.shape)

#plotting
plt.figure(figsize=(10,6))
plt.plot(x_space, y_true, color="red", label="True function")
plt.scatter(x_obs, y_n_1, color="blue", alpha=0.5, label=f"Noise level {noise_levels[0]}")
plt.scatter(x_obs, y_n_2, color="green", alpha=0.5, label=f"Noise level {noise_levels[1]}")
plt.scatter(x_obs, y_n_3, color="purple", alpha=0.5, label=f"Noise level {noise_levels[2]}")
plt.title("Simulated Observations with 3 levels of Noise")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.show()

"""Observation:
- *noise levels for this functions are "acceptable" below 0.5*
- *for noise levels lover than 0.3 i can understand the hidden function*


"""

#let me collect all the noise in a list
noise_db = []
y_bin_list = []


for n in noise_levels:
  y_n = y_clean + rng.normal(0, n, size=x_obs.shape)
  #threshold: either clip + round or w booleans
  noise_db.append(y_n)
  #back to binary (0 or 1)
  y_binary = (y_n > 0.5).astype(int)
  y_bin_list.append(y_binary)

"""While making the block of code above i realised i was creating  a set of random noise (noise_db) that i did not use later.

For training i used completely different set of noisy data (I made an extra step, non needed!)

Now fit a small neural network using scikit learn again, using MLPClassifier(max_iter=5000). Can this model properly learn the underlying function ?

**Assignment**: Use the data you've generated with increasing noise level to train your model. For each noise level, plot the model prediction, how does it compares to the target function?

Observations for the following step (learning from trial and error):
- y_n (noisy data) is made of continuous values because it was created with Normal distribution
- MLPClassifier expects binary labels or course

How to solve:
 threshold the noisy data back to binary values
"""

'''Do not execute!!! Just notes


ENUMERATE
take an iterable -> noise list [0.05, 0.3, 0.8]
return pair_confusion_matrix

without enumerate:
  for i in range(len(noise_levels)):
    noise_level = noise_levels[i]

i indexes colors for the subplot

          (mean of the distribution, std dev, shape)
rng.normal(0, noise_level, size=x_obs.shape)

y_clean will be made of binary values: [0,1,1,0,1]
for a single iteration what will happen is:
  noise = rng.normal(0, 0,3, size=5)
  y_noisy = y_clean (continuos values) + noise
*********

TO BINARY
y_binary = (y_noisy > 0.5).astype(int)
- step A: creation of a boolean array

  y_noisy = [-0.15, 1.42, 0.72, 0.51, 1.13]
  y_noisy > 0.5

  result: F, T, T, T, T

- step B: from boolean to integer

  [result].astype(int)
********
X_obs.shape -> (200,1) 200 observations, 1 feature
y_binary.shape (200,) 200 binary lables

"given x, perdict whether y is 1 or 0"
*******

PREDICTIONS
y_pred = model.predict(X_space)

-> X_space.shape : 200 evely-spaced points
y_pred.shape: for each point, predict 0 or 1


'''

"""In setting up the classifier i have to include the method that helps in splitting training and test set"""

# SET UP THE CLASSIFIER

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

#later i have to evaluate errors
trained = []
#y_bin_list = []
splits =[]

#sklearn expects 2D array so i have to reshape the input with noise
X_obs = x_obs.reshape(-1, 1)
X_space = x_space.reshape(-1,1)

plt.figure(figsize=(22, 6))
colors = ["blue", "purple", "green"]

# METHOD 1: ENUMERATE, ASTYPE

for i, noise_level in enumerate(noise_levels):

  y_binary = y_bin_list[i]


  #train/test split
  X_train, X_test, y_train, y_test = train_test_split(X_obs, y_binary, test_size=0.3, random_state= 33)
  splits.append((X_test, y_test))


  #training
  model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=5000, random_state=24)
  model.fit(X_train, y_train)
  trained.append(model)

  #predictions
  y_pred = model.predict(X_space)

  plt.subplot(1, 3, i+1)
  plt.scatter(x_obs, y_binary, color=colors[i], alpha=0.3, s=30, label=f"Noisy data")
  plt.plot(x_space, y_pred, color=colors[i], linewidth=2, label=f"NN prediction")
  plt.plot(x_space, y_true, color="red", linewidth=2, linestyle="--", label="True function")
  plt.title(f"Noise level: {noise_level}")
  plt.xlabel("x")
  plt.ylabel("y")
  plt.legend()
  plt.ylim(-0.1, 1.1)

plt.suptitle("MLPClassifier predictions vs True Function at Different Noise Levels", fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

# VISUALISATION OPTION 2: INDIVIDUAL PLOTS, MORE CLEAR

noisy_list = [y_n_1, y_n_2, y_n_3]

for i, (y_n, noise_level) in enumerate(zip(noisy_list, noise_levels)):
    plt.figure(figsize=(10, 6))  # Individual figure size

    # same model as before
    model = MLPClassifier(hidden_layer_sizes=(10, 10), random_state=42, max_iter=1000)

    #converting to bianry
    y_binary= (y_n>0.5).astype(int)

    #fitting model
    model.fit(x_obs.reshape(-1, 1), y_binary)

    #making porediction

    y_pred = model.predict(x_space.reshape(-1, 1))

    # Plotting
    plt.scatter(x_obs, y_n, color=colors[i], alpha=0.6, s=50, label='Noisy data')
    plt.plot(x_space, y_true, color='red', linewidth=3, label='True function')
    plt.plot(x_space, y_pred, color='blue', linewidth=3, label='MLP prediction', linestyle='--')

    plt.title(f'MLPClassifier - Noise Level: {noise_level}', fontsize=14)
    plt.xlabel('x', fontsize=12)
    plt.ylabel('y', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# METHOD 2: MANUAL LOOP, NP.WHERE
for i in range(len(noise_levels)):

    noise_level = noise_levels[i]
    y_noisy = y_clean + rng.normal(0, noise_level, size=x_obs.shape)

    y_binary = np.where(y_noisy > 0.5, 1, 0)

    # Training
    model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=5000, random_state=42)
    model.fit(X_obs, y_binary)

    # Predictions
    y_pred = model.predict(X_space)

    plt.subplot(1, 3, i+1)
    plt.scatter(x_obs, y_binary, color=colors[i], alpha=0.3, s=20, label=f"Noisy data")
    plt.plot(x_space, y_pred, color=colors[i], linewidth=2, label=f"NN prediction")
    plt.plot(x_space, y_true, color="red", linewidth=2, linestyle="--", label="True function")
    plt.title(f"Noise level: {noise_level}")
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend()
    plt.ylim(-0.1, 1.1)

plt.suptitle("MLPClassifier Predictions vs True Function at Different Noise Levels", fontsize=14)
plt.tight_layout()
plt.show()

"""Now that we studdy a classification problem, the absolute difference or square difference do not really make sense as a metric.

We can count the error rate. This amounts is the ratio of the predictions where the predicted value and the true value differ.

What is the error rate of your model ?

**Assignment**: For the models you've trained at different noise level, plot the error rate.
"""

# Error rates for each level of noise
err_rates = []


for i, (model, y_binary, noise_level) in enumerate(zip(trained, y_bin_list, noise_levels)):
  X_test, y_test = splits[i]

  y_pred_test = model.predict(X_test)

  # error rate = 1- accuracy
  # error rate = n of wrong predictions/ total predictions
  accuracy = accuracy_score(y_test, y_pred_test)
  error_rate = 1- accuracy

  err_rates.append(error_rate)

  print(f"Noise level {noise_level}: Accuracy = {accuracy:.4f}, Error rate= {error_rate:.4f}")

"""We can observe that the accuracy decreases as the noise level increases.

"""

#Error rate bar chart
plt.figure(figsize=(10,6))
bars = plt.bar(range(len(noise_levels)), err_rates, color=colors, alpha=0.7,edgecolor='black', linewidth=1.5 )

for i, (bar, err_rate) in enumerate(zip(bars, err_rates)):
  height = bar.get_height()
  plt.text(bar.get_x() + bar.get_width()/2., height, f'{err_rate:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.xlabel('Noise Level', fontsize=12, fontweight='bold')
plt.ylabel('Error Rate', fontsize=12, fontweight='bold')
plt.title('Model Error Rate vs Noise Level', fontsize=14, fontweight='bold')
plt.xticks(range(len(noise_levels)), [f'{nl}' for nl in noise_levels], fontsize=11)
plt.ylim(0, max(err_rates) * 1.2)  # Add 20% space above for labels
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

#error rate line plot
plt.figure(figsize=(10,6))
plt.plot(noise_levels, err_rates, marker='o', linestyle='-',
         color='red', linewidth=3)

# Add value labels on each point
for i, (noise, err_rate) in enumerate(zip(noise_levels, err_rates)):
    plt.text(noise, err_rate + 0.01, f'{err_rate:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.xlabel('Noise Level', fontsize=12, fontweight='bold')
plt.ylabel('Error Rate', fontsize=12, fontweight='bold')
plt.title('Error Rate vs Noise Level', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.ylim(0, max(err_rates) * 1.2)  # Add some space above for labels
plt.tight_layout()
plt.show()

"""# A real life machine learning example

Our dataset is from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically **predict whether a patient has diabetes**, based on certain diagnostic measurements included in the dataset.

The dataset consists of several medical predictor variables and one target variable, **Outcome**. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.

Can you build a machine learning model to accurately predict whether the patients in the dataset have diabetes or not?

The data is relatively clean, we won't discuss pre-processing in detail here, besides the very basics.

First, let's **import the libraries** we will need. Then, we need to **read the csv file**.
"""

from google.colab import drive

drive.mount('/content/drive')

import pandas as pd

diabetes = pd.read_csv('/content/drive/MyDrive/diabetes.csv')

"""**Assignment**: Let's print the columns and the few top

*   Voce elenco
*   Voce elenco

values from the dataset, to see what we're working with.
"""

# Print the list of columns or features
diabetes.columns

# Print the top values
diabetes.head()

"""I am visualising the first 5 rows

**Assignment**: Let's take a look at the Outcome variable that we want to predict. **How many people from our dataset have diabetes?**
"""

diabetes.groupby('Outcome').count()

"""In our dataset, 268 people have diabetes, 500 don't .

**Assignment**: Do we have **missing values** in any columns?
"""

diabetes.isnull().sum()

"""No missing values

**Assignment**: Let's look at the **distribution** of each feature to see if we find anything weird.

You might want to use the pandas.DataFrame.hist method (for histogram representation)
"""

#diabetes.hist(figsize=(12, 10))
#plt.tight_layout()
#plt.show()

fig, axes = plt.subplots(3,3, figsize=(15,12))
axes = axes.ravel()

units= {'Pregnancies': 'Number',
    'Glucose': 'mg/dL',
    'BloodPressure': 'mmHg',
    'SkinThickness': 'mm',
    'Insulin': 'Î¼U/mL',
    'BMI': 'kg/mÂ²',
    'DiabetesPedigreeFunction': 'Score',
    'Age': 'Years',
    'Outcome': 'Class (0/1)'}

#1 histogram for each feature
features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
            'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']

for i, feature in enumerate(features):
  axes[i].hist(diabetes[feature], color='skyblue')
  axes[i].set_title(f'Distribution of {feature}')
  axes[i].set_xlabel(f'{feature} ({units[feature]})')  #units of measure for clear understanding
  axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""From the histogram we can see `0` values for *Blood Pressure, BMI, Skin Fold Thikness, Insulin,* and *Blood Glucose*. Those values aren't realistic, and are probably in fact hidden **missing values**.

For *Blood Pressure, BMI,* and *Glucose*, only a **few values** are null. We can safely **drop** those rows.

So: the issue is that some values are '0', but it is not realistic that these medical parameters go to 0. Therefore they could be hidden missing values.
"""

question = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

for col in question:
  zeros = (diabetes[col]==0).sum()
  tot = len(diabetes[col])
  perc = (zeros/tot)*100
  print(f'{col}: {zeros} zeros ({perc:.1f}% of data)')

"""So these false values are actually alot for Skin thickness (almost 30%) and insulin (almost 49%)."""

# dropping weird zeros where the % is small
#'AND' operator
print(diabetes.shape)

diabetes_cl = diabetes[(diabetes['Glucose']!=0)&(diabetes['BloodPressure']!=0)&(diabetes['BMI']!=0)]
print("New dimensions:", diabetes_cl.shape)

#maybe i need to drop with 'OR':
diabetes_cln = diabetes[
    (diabetes['BloodPressure'] != 0) &
    (diabetes['BMI'] != 0) &
    (diabetes['Glucose'] != 0) &
    (diabetes['BloodPressure'].notna()) &
    (diabetes['BMI'].notna()) &
    (diabetes['Glucose'].notna())
]
print('New dimensions:', diabetes_cln.shape)

# Create a new variable diabetes_mod and drop null values
#diabetes_mod = ___

"""It seems like there are a lot of `0` values in the *Insulin* column. Let's check how many."""

ins_zero = diabetes_cln[diabetes_cln['Insulin']==0]
skin_t_zero = diabetes_cln[diabetes_cln['SkinThickness']==0]
insulin_or_skin_zero = diabetes_cln[(diabetes_cln['Insulin'] == 0) | (diabetes_cln['SkinThickness'] == 0)]
print(f"Rows with 0 in 'Insulin': {ins_zero.shape[0]}")
print(f"Rows with 0 in 'SkinThickness': {skin_t_zero.shape[0]}")
print(f"Rows with 0 in either 'Insulin' or 'SkinThickness': {insulin_or_skin_zero.shape[0]}")

"""SkinThickess: from  227 to 192 zeros
Insulin: from 374 to 322 zeros
"""

#done above
#insulin_null = ____
#print(insulin_null)

"""We will **lose a lot of information** if we remove all rows where Insulin value is zero. It seems like a valuable variable to drop it completely. Let's keep it for now and see how it goes.

Now that the data is ready we can **train our model**. Let's try a model called Logistic Regression.

First, we will create `X` variable with the **features**, and `y` variable with the **outcome**. We will need to split the data into **train and test sets**. There is a function `train_test_split` for this. Please google it to find scikit documentation with the example of usage. We will use **accuracy** to get the estimates. It's a good choice because our outcome variable is rather balanced.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

'''diabetes_clean = diabetes[
    (diabetes['Glucose'] != 0) &
    (diabetes['BloodPressure'] != 0) &
    (diabetes['BMI'] != 0)
]

print(diabetes_clean.shape)'''

feature_names = ['Pregnancies', 'Glucose', 'BloodPressure',
                 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction',
                 'Age']

X = diabetes_clean[feature_names]
y = diabetes_clean['Outcome']

#splitting data

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.2, random_state= 43)

"""Now the fun part!
To get our predictions we need to:
1. **Initialize** the model
2. **Fit** the model (using the input feature vectors `X` and the outcome vector `y` of the **training set**)
3. Use the trained model to **predict** outcome values from the **training set**, and then the **test set** (here, we only provide the input features `X`, the model will give us the predicted `y` values)
4. **Compare** the values the model predicted with the actual outcome values. We will use the accuracy_score function. Please google how to use it.
"""

#define and train the model
model = LogisticRegression(random_state=60, max_iter=1000)
model.fit(X_train, y_train)

#training accuracy
y_pred_train = model.predict(X_train)
accuracy_score(y_train, y_pred_train)

y_pred_test = model.predict(X_test) #make predictions on test features
accuracy_score(y_test, y_pred_test) #accuracy on true labels for the test set and predicted labels

"""Training accuracy ~ 78.5% .
Test accuracy ~ 73% .
Loss ~ 5.5%

Now, let's have a look at the confusion matrix.

The confusion matrix is a table used in classification problems to assess where in the models there are errors:
- rows = correct outcome
- columns = prediction outcome
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

conf_matrix = confusion_matrix(y_test, y_pred_test) #confusion_matrix(actual, predicted)
disp = ConfusionMatrixDisplay(conf_matrix)
disp.plot()

"""**Assignment**: What can you say about precision, recall and sensitivity?

Hint: You might want to fetch TN, FP, FN, TP from the confusion matrix.
"""

def confusion_metrics(conf_matrix):
  tn, fp, fn, tp = conf_matrix.ravel()

  #convert to python native types to avoid "seeing the type" in the output
  tn = int(tn)
  fp = int(fp)
  fn = int(fn)
  tp = int(tp)

  #within the function i compute accuracy, precision, recall, specificity and f1 score
  accuracy = float((tp + tn) / (tp + tn + fp + fn))
  precision = float(tp / (tp + fp)) if (tp + fp) > 0 else 0
  recall = float(tp / (tp + fn)) if (tp + fn) > 0 else 0
  specificity = float(tn / (tn + fp)) if (tn + fp) > 0 else 0
  f1_score = float(2 * (precision * recall) / (precision + recall) )if (precision + recall) > 0 else 0

  #then i return these metrics
  print("TN:", tn,
      "\nTP:", tp,
      "\nFN:", fn,
      "\nFP:", fp,
      "\nAccuracy:", accuracy,
      "\nRecall:", recall,
      "\nPrecision:", precision,
      "\nSpecificity:", specificity,
      "\nF1-score:", f1_score)

confusion_metrics(conf_matrix)

"""- accuracy = 0.73
- recall 0.429
- precision = 0.66
- specificity = 0.885
- F1-score = 0.52

the model correctly found 85 negatives (TN) and 21 positives (TP); it failed 28+11 times.

Since we are in a medical diagnostic settings:
- high specificity is good ! because it allows us to get an efficient sorting of patients and avoid further exams
- recall is mediocre, meaning that this model can catch diabetic patients ~ 43% of the time (not all the data tha had to be predicted as true, has been predicted this way)
- overall, accuracy is decent: overall, the predictions made tend to be correct
- precision is mediocre, meaning that out of all the positive prediction, a bit more than half were correct
- f1 score is just slightly bigger than 50%


So: when this model indicates that someone is health, i can trust it; when it says someone has diabetes, i have to be cautious and check better.

## Congratulations on your first model!
The accuracy on the test set is almost as good as the accuracy on the training set. That means that the model doesn't **overfit** to the training set.
Let's try another model, that is famous for overfitting if not tuned well.

We will use the DecisionTreeClassifier.
"""

from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(random_state=100)
tree.fit( diabetes_clean[feature_names] , diabetes_clean["Outcome"] )

y_pred_train = tree.predict(X_train)
accuracy_score(y_train, y_pred_train)

y_pred_test = tree.predict(X_test)
accuracy_score(y_test, y_pred_test)

"""**Assignment**: what do you observe?

I observe that accuracy of the prediction is perfect (1.0) in both cases.
This suggests that the model is **overfitting**

One of the way to prevent overfitting is to force the model to be "**simpler**". For the Decision Tree algorithm, one of the indication of its complexity is its "depth". Let's try limiting it to `3`.
"""

tree = DecisionTreeClassifier(max_depth=3)
tree.fit( diabetes_clean[feature_names] , diabetes_clean["Outcome"] )

y_pred_train = tree.predict(X_train)
accuracy_score(y_train, y_pred_train)

y_pred_test = tree.predict(X_test)
accuracy_score(y_test, y_pred_test)

"""**Assignment**: what do you observe?
Now both values are good but not perfect ans, as expected, accuracy on test is slightly lower than accuracy on training. This suggestst that the model can "abstract"
"""



"""Let's have a look at the confusion matrix !"""

conf_matrix2 = confusion_matrix(y_test, y_pred_test) #confusion_matrix(actual, predicted)
disp = ConfusionMatrixDisplay(conf_matrix2)
disp.plot()

"""**Assigment**: What can you say about precision, recall and sensitivity? Do these results differ from previous ones? How do they differ?"""

confusion_metrics(conf_matrix2)

"""- Accuracy is slightly higher -> the model "was right" a bit more often
- Recall is worse
- Precision improved
- Specificity is 94, so not far from 100%
- F1 score went down a bit

The decison tree became better at identifiying healthy people but is missing more diabetics overall.

For diabetes screening recall is more important so the first model is better. (if we keep tree's depth to 3).
"""